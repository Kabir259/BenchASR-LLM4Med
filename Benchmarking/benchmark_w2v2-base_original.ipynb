{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":302713,"sourceType":"datasetVersion","datasetId":125828}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install transformers datasets evaluate jiwer","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2024-11-23T20:52:04.740210Z","shell.execute_reply.started":"2024-11-23T20:51:53.500197Z","shell.execute_reply":"2024-11-23T20:52:04.739098Z"}},"outputs":[{"name":"stdout","text":"Installing collected packages: rapidfuzz, jiwer, evaluate\nSuccessfully installed evaluate-0.4.3 jiwer-3.0.5 rapidfuzz-3.10.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torchaudio\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    Wav2Vec2Processor,\n    Wav2Vec2ForCTC,\n    Trainer,\n    TrainingArguments,\n    Wav2Vec2CTCTokenizer,\n    Wav2Vec2FeatureExtractor,\n)\nimport evaluate\nimport os\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport numpy as np \n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:52:06.518069Z","iopub.execute_input":"2024-11-23T20:52:06.518432Z","iopub.status.idle":"2024-11-23T20:52:25.699659Z","shell.execute_reply.started":"2024-11-23T20:52:06.518400Z","shell.execute_reply":"2024-11-23T20:52:25.698713Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(new_session=False,\n      write_permission=True, \n      token='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', \n      add_to_git_credential=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:52:25.701435Z","iopub.execute_input":"2024-11-23T20:52:25.702511Z","iopub.status.idle":"2024-11-23T20:52:25.843843Z","shell.execute_reply.started":"2024-11-23T20:52:25.702476Z","shell.execute_reply":"2024-11-23T20:52:25.842963Z"}},"outputs":[{"name":"stdout","text":"Token is valid (permission: write).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"base_path = \"/kaggle/input/medical-speech-transcription-and-intent/Medical Speech, Transcription, and Intent\"\ncsv_file_path = os.path.join(base_path, \"overview-of-recordings.csv\")\nrecordings_path = os.path.join(base_path, \"recordings\")\n\ndf = pd.read_csv(csv_file_path)\n\ndef find_subdirectory_and_path(file_name):\n    for subdirectory in ['test', 'train', 'validate']:\n        file_path = os.path.join(recordings_path, subdirectory, file_name)\n        if os.path.exists(file_path):\n            return subdirectory, file_path\n    return None, None \n\ndf[['subdirectory', 'file_path']] = df['file_name'].apply(\n    lambda file_name: pd.Series(find_subdirectory_and_path(file_name))\n)\ndf = df.drop(['writer_id','speaker_id','file_download','file_name'], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:52:52.562467Z","iopub.execute_input":"2024-11-23T20:52:52.562791Z","iopub.status.idle":"2024-11-23T20:52:57.348339Z","shell.execute_reply.started":"2024-11-23T20:52:52.562753Z","shell.execute_reply":"2024-11-23T20:52:57.347649Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict, Audio\nimport pandas as pd\n\ndataset = Dataset.from_pandas(df)\n\ntrain_dataset = dataset.filter(lambda x: x['subdirectory'] == 'train')\ntest_dataset = dataset.filter(lambda x: x['subdirectory'] == 'test')\nvalidate_dataset = dataset.filter(lambda x: x['subdirectory'] == 'validate')\n\ndataset_dict = DatasetDict({\n    \"train\": train_dataset,\n    \"test\": test_dataset,\n    \"validate\": validate_dataset\n})\n\nfor split in dataset_dict:\n    dataset_dict[split] = dataset_dict[split].cast_column(\"file_path\", Audio())\n    dataset_dict[split] = dataset_dict[split].rename_column(\"file_path\", \"audio\")\n    dataset_dict[split] = dataset_dict[split].rename_column(\"phrase\", \"text\")\n\n\ndata = dataset_dict.remove_columns([\"subdirectory\",\"prompt\",'audio_clipping', 'audio_clipping:confidence',\n                                    'background_noise_audible', 'background_noise_audible:confidence',\n                                    'overall_quality_of_the_audio', 'quiet_speaker', 'quiet_speaker:confidence'])\n\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:52:59.284772Z","iopub.execute_input":"2024-11-23T20:52:59.285146Z","iopub.status.idle":"2024-11-23T20:52:59.629182Z","shell.execute_reply.started":"2024-11-23T20:52:59.285114Z","shell.execute_reply":"2024-11-23T20:52:59.628285Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/6661 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9842e72615a34ad6b79026811ecd9e41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/6661 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d546203c937b44528dc7d0dc97b91f6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/6661 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d78f1aae48e04b43ba461108d3c47a32"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'audio'],\n        num_rows: 381\n    })\n    test: Dataset({\n        features: ['text', 'audio'],\n        num_rows: 5895\n    })\n    validate: Dataset({\n        features: ['text', 'audio'],\n        num_rows: 385\n    })\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import re\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\ï\\`\\√\\d\\\\n]'\n\ndef remove_special_characters(batch):\n    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n    return batch\n    \ndata = data.map(remove_special_characters)\n\ndef extract_all_chars(batch):\n  all_text = \" \".join(batch[\"text\"])\n  vocab = list(set(all_text))\n  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n\nvocabs = data.map(extract_all_chars, batched=True, \n                  batch_size=-1, \n                  keep_in_memory=True, \n                  remove_columns=data.column_names[\"train\"])\n\nvocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))\nvocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\n\nvocab_dict[\"|\"] = vocab_dict[\" \"]\ndel vocab_dict[\" \"]\nvocab_dict[\"[UNK]\"] = len(vocab_dict)\nvocab_dict[\"[PAD]\"] = len(vocab_dict)\n\nimport json\n\nwith open('vocab.json', 'w') as vocab_file:\n    json.dump(vocab_dict, vocab_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:53:01.565407Z","iopub.execute_input":"2024-11-23T20:53:01.566444Z","iopub.status.idle":"2024-11-23T20:53:02.226371Z","shell.execute_reply.started":"2024-11-23T20:53:01.566410Z","shell.execute_reply":"2024-11-23T20:53:02.225507Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/381 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aa78f553dce4766973b9a52be598bf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b4ecc83e47945b3a49c96d8c05df152"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/385 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"503277dd9f0142698330f8b4881bb6dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/381 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"898800a6fc7946e5944edaf2288e8817"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb034ac057b0424787575de0f7f99ae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/385 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d66c735bbbc45088f4fc46da9aac50c"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from transformers import AutoModelForCTC, Wav2Vec2Processor\n# import os\n# os.environ['CURL_CA_BUNDLE'] = ''\n\nmodel_name = \"facebook/wav2vec2-base\"\nprocessor = Wav2Vec2Processor.from_pretrained(model_name)\nmodel = AutoModelForCTC.from_pretrained(model_name).to(\"cuda\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:53:05.006639Z","iopub.execute_input":"2024-11-23T20:53:05.007028Z","iopub.status.idle":"2024-11-23T20:53:09.071649Z","shell.execute_reply.started":"2024-11-23T20:53:05.006994Z","shell.execute_reply":"2024-11-23T20:53:09.070926Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8156186b2da4e98acac3327ec368144"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d8bc469f63469b94ff42fca7ecf8a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf36e046da2a448bbbab9db9df8689f8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:302: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b54ee6a8b0024a6198d4686625f92a8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03086e653afa43eab3f17e755f48f12c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb42656587443fca62b41a04e6f49e7"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def prepare_dataset(batch):\n    audio = batch[\"audio\"]\n    batch = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], text=batch[\"text\"])\n    batch[\"input_length\"] = len(batch[\"input_values\"][0])\n    return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:53:12.178117Z","iopub.execute_input":"2024-11-23T20:53:12.178980Z","iopub.status.idle":"2024-11-23T20:53:12.183332Z","shell.execute_reply.started":"2024-11-23T20:53:12.178944Z","shell.execute_reply":"2024-11-23T20:53:12.182345Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"data = data.cast_column(\"audio\", Audio(sampling_rate=16_000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:53:14.692026Z","iopub.execute_input":"2024-11-23T20:53:14.692970Z","iopub.status.idle":"2024-11-23T20:53:14.701996Z","shell.execute_reply.started":"2024-11-23T20:53:14.692920Z","shell.execute_reply":"2024-11-23T20:53:14.701035Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data = data.map(prepare_dataset, remove_columns=data.column_names[\"validate\"], num_proc=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:53:15.531637Z","iopub.execute_input":"2024-11-23T20:53:15.532502Z","iopub.status.idle":"2024-11-23T20:55:03.925235Z","shell.execute_reply.started":"2024-11-23T20:53:15.532468Z","shell.execute_reply":"2024-11-23T20:55:03.924328Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/381 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24dc0745b11e4104bd1e1378fb42126e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49b5f5707b8346b2b53b803e01f29e44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/385 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b37ee4654044989c5b913b16453b41"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"def map_to_result(batch):\n    with torch.no_grad():\n        input_values = torch.tensor(batch[\"input_values\"][0], device=\"cuda\")  #  Extract the audio data from the list\n        print(f\"Shape after unpacking and converting to tensor: {input_values.shape}\")  # Should be [sequence_length]\n\n        input_values = input_values.unsqueeze(0)  # Add batch dimension\n\n        logits = model(input_values).logits  # Shape: [1, sequence_length, vocab_size]\n\n        pred_ids = torch.argmax(logits, dim=-1)  # Shape: [1, sequence_length]\n        batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n        batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n\n    return batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:55:08.516348Z","iopub.execute_input":"2024-11-23T20:55:08.516729Z","iopub.status.idle":"2024-11-23T20:55:08.523242Z","shell.execute_reply.started":"2024-11-23T20:55:08.516689Z","shell.execute_reply":"2024-11-23T20:55:08.522284Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"results = data[\"validate\"].map(map_to_result, remove_columns=data[\"validate\"].column_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\nwer_metric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_logits = pred.predictions\n    pred_ids = np.argmax(pred_logits, axis=-1)\n\n    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n\n    pred_str = processor.batch_decode(pred_ids)\n    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n\n    wer = float('inf') # init wer with a default value\n\n    try:\n        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n    except Exception as e:\n        print(f\"Error computing WER. Predictions: {pred_str}, References: {label_str}, Error: {e}\")\n\n    return {\"wer\": wer}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:55:59.869935Z","iopub.execute_input":"2024-11-23T20:55:59.870665Z","iopub.status.idle":"2024-11-23T20:56:00.569031Z","shell.execute_reply.started":"2024-11-23T20:55:59.870630Z","shell.execute_reply":"2024-11-23T20:56:00.568248Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9530cdf193ed47b5a22144043cb920f9"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"print(\"w2v2-base_original Val WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:56:05.288655Z","iopub.execute_input":"2024-11-23T20:56:05.289137Z","iopub.status.idle":"2024-11-23T20:56:05.328353Z","shell.execute_reply.started":"2024-11-23T20:56:05.289101Z","shell.execute_reply":"2024-11-23T20:56:05.327412Z"}},"outputs":[{"name":"stdout","text":"w2v2-base_original Val WER: 1.352\n","output_type":"stream"}],"execution_count":15}]}