{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":302713,"sourceType":"datasetVersion","datasetId":125828}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -q transformers datasets evaluate jiwer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:02:14.902629Z","iopub.execute_input":"2024-11-25T03:02:14.902989Z","iopub.status.idle":"2024-11-25T03:02:26.872980Z","shell.execute_reply.started":"2024-11-25T03:02:14.902950Z","shell.execute_reply":"2024-11-25T03:02:26.872016Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torchaudio\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    WhisperTokenizer,\n    WhisperProcessor,\n    WhisperFeatureExtractor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\nimport evaluate\nimport os\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport numpy as np \n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:02:26.875103Z","iopub.execute_input":"2024-11-25T03:02:26.875502Z","iopub.status.idle":"2024-11-25T03:02:47.084295Z","shell.execute_reply.started":"2024-11-25T03:02:26.875457Z","shell.execute_reply":"2024-11-25T03:02:47.083403Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(new_session=False,\n      write_permission=True, \n      token='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', \n      add_to_git_credential=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:02:47.085492Z","iopub.execute_input":"2024-11-25T03:02:47.086572Z","iopub.status.idle":"2024-11-25T03:02:47.323823Z","shell.execute_reply.started":"2024-11-25T03:02:47.086527Z","shell.execute_reply":"2024-11-25T03:02:47.322812Z"}},"outputs":[{"name":"stdout","text":"Token is valid (permission: write).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom datasets import Dataset, Audio\n\n# Define base paths\nbase_path = \"/kaggle/input/medical-speech-transcription-and-intent/Medical Speech, Transcription, and Intent\"\ncsv_file_path = os.path.join(base_path, \"overview-of-recordings.csv\")\nrecordings_path = os.path.join(base_path, \"recordings\")\n\n# Load CSV\ndf = pd.read_csv(csv_file_path)\n\n# Function to find the subdirectory and file path\ndef find_subdirectory_and_path(file_name):\n    for subdirectory in ['test', 'train', 'validate']:\n        file_path = os.path.join(recordings_path, subdirectory, file_name)\n        if os.path.exists(file_path):\n            return subdirectory, file_path\n    return None, None \n\n# Apply the function to find subdirectories and paths\ndf[['subdirectory', 'file_path']] = df['file_name'].apply(\n    lambda file_name: pd.Series(find_subdirectory_and_path(file_name))\n)\n\n# Drop unnecessary columns\ndf = df.drop(['writer_id', 'speaker_id', 'file_download', 'file_name'], axis=1)\n\n# Convert dataframe to dataset\ndataset = Dataset.from_pandas(df)\n\n# Split the dataset into train, test, and validate\ntrain_data = dataset.filter(lambda x: x['subdirectory'] == 'train')\ntest_data = dataset.filter(lambda x: x['subdirectory'] == 'test')\nval_data = dataset.filter(lambda x: x['subdirectory'] == 'validate')\n\n# Cast and rename columns for each split\nfor split_name, split_data in zip(['train', 'test', 'validate'], [train_data, test_data, val_data]):\n    split_data = split_data.cast_column(\"file_path\", Audio())\n    split_data = split_data.rename_column(\"file_path\", \"audio\")\n    split_data = split_data.rename_column(\"phrase\", \"text\")\n    if split_name == 'train':\n        train_data = split_data\n    elif split_name == 'test':\n        test_data = split_data\n    elif split_name == 'validate':\n        val_data = split_data\n\n# Remove unnecessary columns from each split\ncolumns_to_remove = [\n    \"subdirectory\", \"prompt\", 'audio_clipping', 'audio_clipping:confidence',\n    'background_noise_audible', 'background_noise_audible:confidence',\n    'overall_quality_of_the_audio', 'quiet_speaker', 'quiet_speaker:confidence'\n]\n\ntrain_data = train_data.remove_columns(columns_to_remove)\ntest_data = test_data.remove_columns(columns_to_remove)\nval_data = val_data.remove_columns(columns_to_remove)\n\n# Format preview for a single sample\ndef preview_sample(dataset):\n    sample = dataset[0]  # First sample in the dataset\n    return {\n        \"audio\": {\n            \"path\": sample[\"audio\"][\"path\"],\n            \"array\": sample[\"audio\"][\"array\"],  # No need to convert; it's already a list or numpy array\n            \"sampling_rate\": sample[\"audio\"][\"sampling_rate\"],\n        },\n        \"text\": sample[\"text\"]\n    }\n\nprint(\"Train Data:\", train_data)\ntrain_sample = preview_sample(train_data)\nprint(\"Train Sample:\", train_sample)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:02:47.325321Z","iopub.execute_input":"2024-11-25T03:02:47.325985Z","iopub.status.idle":"2024-11-25T03:03:24.233698Z","shell.execute_reply.started":"2024-11-25T03:02:47.325946Z","shell.execute_reply":"2024-11-25T03:03:24.232825Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/6661 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"535510595ae84b958a9de9e8914c373d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/6661 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2fca11786784faf93e954f3ab162ddf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/6661 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b9c8cdb031a4539917d8cd44f444820"}},"metadata":{}},{"name":"stdout","text":"Train Data: Dataset({\n    features: ['text', 'audio'],\n    num_rows: 381\n})\nTrain Sample: {'audio': {'path': '/kaggle/input/medical-speech-transcription-and-intent/Medical Speech, Transcription, and Intent/recordings/train/1249120_44197979_23991689.wav', 'array': array([0.02072144, 0.01501465, 0.01168823, ..., 0.05114746, 0.10168457,\n       0.07489014]), 'sampling_rate': 48000}, 'text': 'I have a sharp pain in my lower stomach.'}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_data = train_data.cast_column(\"audio\", Audio(sampling_rate=16000))\ntest_data = test_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nval_data = val_data.cast_column(\"audio\", Audio(sampling_rate=16000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:03:24.235787Z","iopub.execute_input":"2024-11-25T03:03:24.236406Z","iopub.status.idle":"2024-11-25T03:03:24.245348Z","shell.execute_reply.started":"2024-11-25T03:03:24.236375Z","shell.execute_reply":"2024-11-25T03:03:24.244473Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import re\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\ï\\`\\√\\d\\\\n]'\n\ndef remove_special_characters(batch):\n    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n    return batch\n    \ntrain_data = train_data.map(remove_special_characters)\ntest_data = test_data.map(remove_special_characters)\nval_data = val_data.map(remove_special_characters)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:03:24.246499Z","iopub.execute_input":"2024-11-25T03:03:24.246857Z","iopub.status.idle":"2024-11-25T03:03:24.806873Z","shell.execute_reply.started":"2024-11-25T03:03:24.246820Z","shell.execute_reply":"2024-11-25T03:03:24.806109Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/381 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd43e843cb24bf88e942c752b1a8ad3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5895 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632eccfe39ab4f03849eaffcb5d7881d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/385 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a66b8717894f6984d3065e6ceac8a5"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"val_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:10:56.016943Z","iopub.execute_input":"2024-11-25T03:10:56.017592Z","iopub.status.idle":"2024-11-25T03:10:56.023594Z","shell.execute_reply.started":"2024-11-25T03:10:56.017561Z","shell.execute_reply":"2024-11-25T03:10:56.022806Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'audio'],\n    num_rows: 385\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# For FINETUNED Model:","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperProcessor, WhisperForConditionalGeneration\nimport evaluate\n\n# Load the finetuned model and processor\nmodel = WhisperForConditionalGeneration.from_pretrained(\"Kabir259/whisper-small_kabir\")\nprocessor = WhisperProcessor.from_pretrained(\"Kabir259/whisper-small_kabir\")\nmetric = evaluate.load(\"wer\")\nmodel.generation_config.task = \"transcribe\"\n\n# Move model to device\nmodel = model.to(DEVICE)\n\n# Function to prepare predictions\ndef compute_validation_wer(dataset):\n    pred_strs = []\n    label_strs = []\n        \n    for idx, example in enumerate(dataset):\n        try:\n           \n            audio = example[\"audio\"]\n\n            input_features = processor.feature_extractor(\n                audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\"\n            ).input_features[0]  # Select the first feature array (consistent with training)\n\n            input_features = input_features.to(DEVICE)  # Move input to the same device as the model\n\n            labels = example[\"text\"]  # Reference text\n\n            # Generate predictions\n            with torch.no_grad():\n                outputs = model.generate(input_features.unsqueeze(0))  # Add batch dimension\n            pred_str = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n            pred_strs.append(pred_str)\n            label_strs.append(labels)\n        except Exception as e:\n            print(f\"Error processing sample {idx + 1}: {e}\")\n            continue\n\n    # Compute WER\n    try:\n        wer = metric.compute(predictions=pred_strs, references=label_strs)\n        print(f\"WER computed successfully.\")\n    except Exception as e:\n        print(f\"Error during WER computation: {e}\")\n        wer = None\n    \n    return wer\n\n# Evaluate on validation set\ntry:\n    val_wer = compute_validation_wer(val_data)\n    if val_wer is not None:\n        print(f\"Validation WER: {val_wer:.3f}\")\n    else:\n        print(\"Failed to compute WER.\")\nexcept Exception as e:\n    print(f\"Error during evaluation: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:13:57.690263Z","iopub.execute_input":"2024-11-25T03:13:57.690641Z","iopub.status.idle":"2024-11-25T03:16:07.405295Z","shell.execute_reply.started":"2024-11-25T03:13:57.690606Z","shell.execute_reply":"2024-11-25T03:16:07.404404Z"}},"outputs":[{"name":"stdout","text":"WER computed successfully.\nValidation WER: 0.213\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# For BASE model:","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperTokenizer, WhisperFeatureExtractor\nimport evaluate\n\n# Base model details\nmodel_id = \"openai/whisper-small\"\nmodel = WhisperForConditionalGeneration.from_pretrained(model_id)\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(model_id)\ntokenizer = WhisperTokenizer.from_pretrained(model_id, language=\"English\", task=\"transcribe\")\nprocessor = WhisperProcessor.from_pretrained(model_id, language=\"English\", task=\"transcribe\")\n\nmetric = evaluate.load(\"wer\")\n\n# Move model to device\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(DEVICE)\n\n# Function to prepare predictions\ndef compute_validation_wer(dataset):\n    pred_strs = []\n    label_strs = []\n    \n    for idx, example in enumerate(dataset):\n        try:\n        \n            audio = example[\"audio\"]\n\n            input_features = processor.feature_extractor(\n                audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\"\n            ).input_features[0]  # Select the first feature array (consistent with training)\n\n            input_features = input_features.to(DEVICE)  # Move input to the same device as the model\n\n            labels = example[\"text\"]  # Reference text\n\n            # Generate predictions\n            with torch.no_grad():\n                outputs = model.generate(input_features.unsqueeze(0))  # Add batch dimension\n            pred_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n            pred_strs.append(pred_str)\n            label_strs.append(labels)\n        except Exception as e:\n            print(f\"Error processing sample {idx + 1}: {e}\")\n            continue\n\n    # Compute WER\n    try:\n        wer = metric.compute(predictions=pred_strs, references=label_strs)\n        print(f\"WER computed successfully.\")\n    except Exception as e:\n        print(f\"Error during WER computation: {e}\")\n        wer = None\n    \n    return wer\n\n# Evaluate on validation set\ntry:\n    val_wer = compute_validation_wer(val_data)\n    if val_wer is not None:\n        print(f\"Validation WER (Base Model): {val_wer:.2f}\")\n    else:\n        print(\"Failed to compute WER.\")\nexcept Exception as e:\n    print(f\"Error during evaluation: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:21:08.445608Z","iopub.execute_input":"2024-11-25T03:21:08.446000Z","iopub.status.idle":"2024-11-25T03:27:01.786541Z","shell.execute_reply.started":"2024-11-25T03:21:08.445967Z","shell.execute_reply":"2024-11-25T03:27:01.785645Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf19fefcd4554a9aaaf65a4bac180209"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f78bb39fa00c4608a9c2b17c8509de62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4fe9579974a4fc9af7e0486f92b3bb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e528582930547de91902232b2ec6f76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af648eae9be476b980653f9a8bc7a05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ec999ebc804161a856718746a9e447"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc19c5d4a8d542ab9c6e06a4c3fdfc77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3820504542aa433d8fc66357db3cd147"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4688eae3c2824df293f8c1874d4c0db2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b72e5b534e2f4320b3920f9c8db41283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1b89bd9cac34afc910f237555fc4867"}},"metadata":{}},{"name":"stderr","text":"Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n","output_type":"stream"},{"name":"stdout","text":"WER computed successfully.\nValidation WER (Base Model): 1.28\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}